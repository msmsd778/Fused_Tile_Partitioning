{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msmsd778/Fused_Tile_Partitioning/blob/main/FTP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of Fused Tile Partitioning (FTP) introduced in the paper titled DeepThings: Distributed Adaptive Deep Learning Inference on Resource-Constrained IoT Edge Clusters"
      ],
      "metadata": {
        "id": "yWi7WAK0c6Qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://ieeexplore.ieee.org/ielaam/43/8496924/8493499-aam.pdf"
      ],
      "metadata": {
        "id": "dEpgiSKodaxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torchvision import models, transforms\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "import requests\n",
        "import psutil"
      ],
      "metadata": {
        "id": "FnM4E8QnKBzh"
      },
      "execution_count": 424,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First We define required functions and modified classes. This snippet defines a function called get_layer_properties that takes a neural network model as input and returns a list of tuples containing properties (kernel size, stride, padding) for convolutional and pooling layers within the model. The function iterates through the child layers of the input model, and for each convolutional (nn.Conv2d) or max pooling (nn.MaxPool2d) layer encountered, it extracts the layer's kernel size, stride, and padding. The function skips the properties of the first layer by setting the skip_first_layer flag. If a layer is of type nn.Sequential, the function recursively calls itself to extract properties from nested layers. The final list of layer properties is then returned."
      ],
      "metadata": {
        "id": "PciVBfb9U4f9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get layer properties (kernel size, stride, padding) for convolutional and pooling layers\n",
        "def get_layer_properties(model):\n",
        "    properties = []\n",
        "    skip_first_layer = True  # Skipping the properties of the first layer\n",
        "    for layer in model.children():\n",
        "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.MaxPool2d):\n",
        "            if skip_first_layer:\n",
        "                skip_first_layer = False\n",
        "                continue\n",
        "            kernel_size = layer.kernel_size[0] if isinstance(layer.kernel_size, tuple) else layer.kernel_size\n",
        "            stride = layer.stride[0] if isinstance(layer.stride, tuple) else layer.stride\n",
        "            padding = layer.padding[0] if isinstance(layer.padding, tuple) else layer.padding\n",
        "            properties.append((kernel_size, stride, padding))\n",
        "        elif isinstance(layer, nn.Sequential):\n",
        "            properties.extend(get_layer_properties(layer))\n",
        "    return properties"
      ],
      "metadata": {
        "id": "AeIkoqNgPR1p"
      },
      "execution_count": 425,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code defines a function named calculate_input_offsets that takes four coordinates (x1, y1, x2, y2), a neural network model, and the index of a layer within that model as input. It calculates and returns the input offsets for the specified layer based on the given coordinates and the properties of the previous layer (retrieved from the layer_properties list). The function distinguishes between convolutional and pooling layers. Based on the section IV of Deepthings paper, for convolutional layers, it computes the input offsets by considering the kernel size, stride, and the specified coordinates. For pooling layers (specifically max pooling, as indicated by the check for nn.MaxPool2d), it adjusts the offsets accordingly. The resulting input offsets are then returned as four values: x1l_minus1, y1l_minus1, x2l_minus1, and y2l_minus1."
      ],
      "metadata": {
        "id": "YRGJvPGAVEss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the input offsets for a given layer\n",
        "def calculate_input_offsets(x1, y1, x2, y2, model, layer_idx):\n",
        "    kernel_size, stride, _ = layer_properties[layer_idx - 1]\n",
        "\n",
        "    # For convolutional layers\n",
        "    x1l_minus1 = max(0, stride * x1 - kernel_size // 2)\n",
        "    y1l_minus1 = max(0, stride * y1 - kernel_size // 2)\n",
        "    x2l_minus1 = min(stride * x2 + kernel_size // 2, layer_properties[layer_idx - 1][0] - 1)\n",
        "    y2l_minus1 = min(stride * y2 + kernel_size // 2, layer_properties[layer_idx - 1][1] - 1)\n",
        "\n",
        "    # For pooling layers\n",
        "    if isinstance(model.features[layer_idx - 1], nn.MaxPool2d):\n",
        "        x1l_minus1 = stride * x1\n",
        "        y1l_minus1 = stride * y1\n",
        "        x2l_minus1 = min(stride * x2 + stride - 1, layer_properties[layer_idx - 1][0] - 1)\n",
        "        y2l_minus1 = min(stride * y2 + stride - 1, layer_properties[layer_idx - 1][1] - 1)\n",
        "\n",
        "    return x1l_minus1, y1l_minus1, x2l_minus1, y2l_minus1"
      ],
      "metadata": {
        "id": "GMGxP7hpOsLy"
      },
      "execution_count": 426,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function partition_image_with_ftp that partitions an input image tensor into a grid with Fused Tile Partitioning (FTP), considering specified partitions (M, N) and a neural network model. It calculates initial grid boundaries, adjusts them with an overlap factor, and iterates through each partition. For each partition, it recursively computes input offsets through neural network layers. The function extracts corresponding sub-regions from the image tensor, creating a list of partitioned tensors. The result is a grid of image partitions suitable for processing through the neural network with FTP and overlap."
      ],
      "metadata": {
        "id": "by5y4xbCVYif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to partition the image into a grid with FTP\n",
        "def partition_image_with_ftp(image, M, N, model):\n",
        "    _, _, H, W = image.shape\n",
        "\n",
        "    # Calculate output offsets\n",
        "    x1L = [(W * j) // M for j in range(M)]\n",
        "    y1L = [(H * i) // N for i in range(N)]\n",
        "    x2L = [(W * (j + 1)) // M for j in range(M)]\n",
        "    y2L = [(H * (i + 1)) // N for i in range(N)]\n",
        "\n",
        "    partitions = []\n",
        "    for i in range(N):\n",
        "        for j in range(M):\n",
        "            x1L[j] = max((W * j) // M - overlap, 0)\n",
        "            y1L[i] = max((H * i) // N - overlap, 0)\n",
        "            x2L[j] = min((W * (j + 1)) // M + overlap, W)\n",
        "            y2L[i] = min((H * (i + 1)) // N + overlap, H)\n",
        "\n",
        "            # Recursive backward traversal to calculate required tile region\n",
        "            x1, y1, x2, y2 = x1L[j], y1L[i], x2L[j] - 1, y2L[i] - 1\n",
        "            for l in range(len(layer_properties), 0, -1):\n",
        "                x1, y1, x2, y2 = calculate_input_offsets(x1, y1, x2, y2, model, l)\n",
        "\n",
        "            # Calculate start and end points with overlap\n",
        "            start_h = max(y1 - overlap, 0)\n",
        "            end_h = min(y2 + overlap, H)\n",
        "            start_w = max(x1 - overlap, 0)\n",
        "            end_w = min(x2 + overlap, W)\n",
        "            partition = image[:, :, start_h:end_h, start_w:end_w]\n",
        "            partitions.append(partition)\n",
        "\n",
        "    return partitions"
      ],
      "metadata": {
        "id": "0KaIkRtROo8S"
      },
      "execution_count": 427,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a modified ResNet18 model, ModifiedResNet18, by excluding the last two layers (average pooling and fully connected) from the original model. It inherits from nn.Module, initializes a nn.Sequential module (self.features) with the modified layers, and provides a forward method for applying these layers to input tensors. This modification enables obtaining intermediate feature maps from the ResNet18 model."
      ],
      "metadata": {
        "id": "NWDoK1QTWO5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified ResNet Model to allow intermediate outputs\n",
        "class ModifiedResNet18(nn.Module):\n",
        "    def __init__(self, original_model):\n",
        "        super(ModifiedResNet18, self).__init__()\n",
        "        self.features = nn.Sequential(*list(original_model.children())[:-2])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "akKA81EdPlIx"
      },
      "execution_count": 428,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code defines a class called RemainingLayers, which is a module inheriting from nn.Module. It takes an original_model as input during initialization and retains remaining layers such as the average pooling (avgpool) and fully connected (fc) layers from that model. The forward method applies the average pooling, flattens the output tensor, and passes it through the fully connected layer. The result is then returned. This class essentially captures the remaining layers after feature extraction in a given model."
      ],
      "metadata": {
        "id": "uO2Xyd7vWodv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RemainingLayers(nn.Module):\n",
        "    def __init__(self, original_model):\n",
        "        super(RemainingLayers, self).__init__()\n",
        "        self.avgpool = original_model.avgpool\n",
        "        self.fc = original_model.fc\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "NgcEYumdTXyn"
      },
      "execution_count": 429,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code initializes an empty dictionary named intermediate_outputs and defines a function called forward_pass_with_reuse. This function takes the model and a list of input partitions. It iterates through the partitions, printing the shape of each partition for the sake of clarity, and performs a forward pass using the model. The function checks if the intermediate output for the current partition index (idx) is already stored in intermediate_outputs. If found, it reuses the stored output; otherwise, it computes the output using the model and stores it in the dictionary for potential future reuse. The function returns a list of outputs corresponding to each input partition."
      ],
      "metadata": {
        "id": "w9utRBARWvis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intermediate_outputs = {}\n",
        "\n",
        "# Modified function to perform forward pass and store intermediate outputs\n",
        "def forward_pass_with_reuse(model, partitions):\n",
        "    outputs = []\n",
        "    for idx, partition in enumerate(partitions):\n",
        "        print(f\"Partition {idx} shape: {partition.shape}\")\n",
        "        if idx in intermediate_outputs:\n",
        "            output = intermediate_outputs[idx]\n",
        "        else:\n",
        "            output = model(partition)\n",
        "            intermediate_outputs[idx] = output\n",
        "        outputs.append(output)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "aDYNqb-uMKAo"
      },
      "execution_count": 430,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Next two snippets first checks whether a GPU is available and assigns the corresponding device. If the device is a GPU (cuda), it also prints GPU information using !nvidia-smi. If the device is a CPU (cpu), it retrieves information about CPU memory using the psutil library and prints the total, available, used, and percentage of CPU memory."
      ],
      "metadata": {
        "id": "_oDE2DBmW9Ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FxPCCwJ4xPj",
        "outputId": "f2921540-6514-4072-893a-7b4102f8982b"
      },
      "execution_count": 431,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 431
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if device.type == 'cuda':\n",
        "  !nvidia-smi\n",
        "elif device.type == 'cpu':\n",
        "  cpu_memory = psutil.virtual_memory()\n",
        "  print(f\"Total CPU Memory: {cpu_memory.total / (1024 ** 3):.2f} GB\")\n",
        "  print(f\"Available CPU Memory: {cpu_memory.available / (1024 ** 3):.2f} GB\")\n",
        "  print(f\"Used CPU Memory: {cpu_memory.used / (1024 ** 3):.2f} GB\")\n",
        "  print(f\"CPU Memory Usage Percentage: {cpu_memory.percent:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn5aftoK5168",
        "outputId": "17977c38-5cf2-4867-b892-26451bd837dc"
      },
      "execution_count": 432,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total CPU Memory: 12.68 GB\n",
            "Available CPU Memory: 11.48 GB\n",
            "Used CPU Memory: 0.92 GB\n",
            "CPU Memory Usage Percentage: 9.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function calculate_grid_size(device) to determine the grid size based on the available memory of the given computing device. For GPUs, it checks if the total memory is below a threshold and returns (2, 2) if true. For CPUs, it checks if the total memory is below a threshold and also returns (2, 2) if true. Otherwise, it defaults to a grid size of (4, 4). The function is designed for dynamic adjustment of the grid size based on available memory."
      ],
      "metadata": {
        "id": "LVA6gKS7Xq2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_grid_size(device):\n",
        "    if device.type == 'cuda':\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory if device.type == 'cuda' else 0\n",
        "        threshold_gpu_memory = 1200 * 1024**2 # Setting GPU threshold to 1200 MiB\n",
        "        if gpu_memory < threshold_gpu_memory:\n",
        "            return 2, 2\n",
        "    elif device.type == 'cpu':\n",
        "        cpu_memory = psutil.virtual_memory().total\n",
        "        threshold_cpu_memory = 4 * 1024**3  # Setting CPU threshold to 4 GiB\n",
        "        if cpu_memory < threshold_cpu_memory:\n",
        "            return 2, 2\n",
        "\n",
        "    # Default grid size\n",
        "    return 4, 4"
      ],
      "metadata": {
        "id": "BQ22zHmGTSo6"
      },
      "execution_count": 433,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we load the pre-trained ResNet18 model, move it to the specified device, create a modified version using the ModifiedResNet18 class, and print the modified model to see the entire architecture."
      ],
      "metadata": {
        "id": "OnUqkjXvaTYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_model = models.resnet18(pretrained=True).to(device)\n",
        "modified_model = ModifiedResNet18(original_model).to(device)\n",
        "print(modified_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9jdIJQnuGB9",
        "outputId": "d658583e-69ab-43b6-b00d-82e0ee7a36f7"
      },
      "execution_count": 434,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModifiedResNet18(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (7): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally the next code snippets downloads an image and sets up a modified ResNet18 model with intermediate output reuse. It uses torchvision to get the original and modified models, extracts layer properties, and preprocesses the image. The FTP strategy partitions the image, and a parallel model is created. The forward pass is performed, and the remaining layers process the mean output. The predicted class label is obtained using softmax."
      ],
      "metadata": {
        "id": "LnatTxj1YTFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get layer properties for the modified_model, skipping the first layer\n",
        "layer_properties = get_layer_properties(modified_model)[1:]"
      ],
      "metadata": {
        "id": "CBbBeRbjwYxL"
      },
      "execution_count": 435,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GhHeFni_QiT",
        "outputId": "218243eb-95d3-4f7b-db1a-7a9cceef4b4e"
      },
      "execution_count": 436,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "\n",
        "image_url = 'https://media.istockphoto.com/id/877369086/photo/lion-panthera-leo-10-years-old-isolated-on-white.jpg?s=612x612&w=0&k=20&c=J__Jx_BX_FN7iehO965TJtPFYUl0A-bwFgIYaK32R3Y='\n",
        "# image_url = 'https://i.guim.co.uk/img/media/c67da314f21e43b027db4fd9525ab4047cd5d358/76_188_1940_1164/master/1940.jpg?width=1200&height=900&quality=85&auto=format&fit=crop&s=76e6bdd3a91c0313c698cabd7c1e361f'\n",
        "image_path = wget.download(image_url)"
      ],
      "metadata": {
        "id": "GQpZUqUo9xSj"
      },
      "execution_count": 437,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imagenet_labels_url = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
        "imagenet_labels = requests.get(imagenet_labels_url).json()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "modified_model.eval()\n",
        "original_model.eval()\n",
        "\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "image = transform(image)\n",
        "image = Variable(image.unsqueeze(0))\n",
        "\n",
        "overlap = 10\n",
        "M, N = calculate_grid_size(device)\n",
        "partitions = partition_image_with_ftp(image, M, N, modified_model)\n",
        "\n",
        "\n",
        "\n",
        "# In case of using multiple edge devices/nodes with Nvidia GPUs. Further implementation is needed in addition to this.\n",
        "# parallel_model = nn.parallel.DistributedDataParallel(\n",
        "#     modified_model, device_ids=[torch.cuda.current_device()]\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "# In case of using mutiple GPUs. Used to demonstrate the implementation in Colab\n",
        "parallel_model = nn.DataParallel(modified_model)\n",
        "\n",
        "# Test the model with the single transformed image\n",
        "with torch.no_grad():\n",
        "    outputs = forward_pass_with_reuse(parallel_model, partitions)\n",
        "\n",
        "remaining_layers = RemainingLayers(original_model)\n",
        "remaining_layers.eval()\n",
        "\n",
        "mean_output = torch.stack(outputs).mean(dim=0)\n",
        "final_output = remaining_layers(mean_output)\n",
        "\n",
        "softmax = nn.Softmax(dim=1)\n",
        "probabilities = softmax(final_output)\n",
        "\n",
        "predicted_class = torch.argmax(probabilities, dim=1)\n",
        "predicted_label = imagenet_labels[predicted_class.item()]\n",
        "\n",
        "print(f\"Predicted Class: {predicted_label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HTHVQHtKIz3",
        "outputId": "5c81a2dc-9d05-4da2-c531-a630df05b929"
      },
      "execution_count": 438,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partition 0 shape: torch.Size([1, 3, 75, 75])\n",
            "Partition 1 shape: torch.Size([1, 3, 75, 95])\n",
            "Partition 2 shape: torch.Size([1, 3, 75, 95])\n",
            "Partition 3 shape: torch.Size([1, 3, 75, 76])\n",
            "Partition 4 shape: torch.Size([1, 3, 95, 75])\n",
            "Partition 5 shape: torch.Size([1, 3, 95, 95])\n",
            "Partition 6 shape: torch.Size([1, 3, 95, 95])\n",
            "Partition 7 shape: torch.Size([1, 3, 95, 76])\n",
            "Partition 8 shape: torch.Size([1, 3, 95, 75])\n",
            "Partition 9 shape: torch.Size([1, 3, 95, 95])\n",
            "Partition 10 shape: torch.Size([1, 3, 95, 95])\n",
            "Partition 11 shape: torch.Size([1, 3, 95, 76])\n",
            "Partition 12 shape: torch.Size([1, 3, 76, 75])\n",
            "Partition 13 shape: torch.Size([1, 3, 76, 95])\n",
            "Partition 14 shape: torch.Size([1, 3, 76, 95])\n",
            "Partition 15 shape: torch.Size([1, 3, 76, 76])\n",
            "Predicted Class: lion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As previously stated, we displayed the dimensions of each partition. Each tensor includes four elements: [batch size, number of channels, height, width]."
      ],
      "metadata": {
        "id": "JdUWWG1tbTmN"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4HJNY1FjlxZjNBJHHTi9s",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}